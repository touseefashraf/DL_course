{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7KglWQgn6lS"
      },
      "source": [
        "# DL Lab 1.2 - Homework - Implementing a Multi-layer Perceptron in NumPy\n",
        "\n",
        "In this notebook you will implement a small Multi-layer Perceptron (MLP) to distinguish images of flowers from images of leaves. Your MLP will have two layers, and you will be using NumPy for the implementation.\n",
        "\n",
        "***\n",
        "\n",
        "**After completing this homework you will**\n",
        "\n",
        "- Be able to implement **dense** and **sigmoid activation** layers\n",
        "- Be able to design **neural network architectures** containing dense layers and sigmoid activation\n",
        "- Be able to **forward** images through your neural network\n",
        "- Be able to compute the **binary cross-entropy loss** for measuring the network's prediction error\n",
        "- Be able to perform **backpropagation** using the chain rule\n",
        "\n",
        "***\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- You'll be using Python 3 in the iPython based Google Colaboratory\n",
        "- Lines encapsulated in \"<font color='green'>`### START YOUR CODE HERE ###`</font>\" and \"<font color='green'>`### END YOUR CODE HERE ###`</font>\", or marked by \"<font color='green'>`# TODO`</font>\", denote the code fragments to be completed by you.\n",
        "- There's no need to write any other code.\n",
        "- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n",
        "- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" in the \"<font color='green'>`# TODO`</font>\" comments to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
        "- If you get stuck, check your Lecture and Lab notes and use the [discussion forum](https://moodle2.tu-ilmenau.de/mod/forum/view.php?id=4466) in Moodle.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spo74hxU3B7T"
      },
      "source": [
        "<font color='darkblue'> \n",
        "**Remember**  \n",
        "- Run your cells using SHIFT + ENTER (or \"Run cell\")\n",
        "- Write code in the designated areas using Python 3 only\n",
        "- Do not modify the code outside of the designated areas\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjCeYQjBbTt6"
      },
      "source": [
        "## 0 - Packages\n",
        "At first, let's import the packages we will need today:\n",
        "\n",
        "- [numpy](www.numpy.org) remember our first lab: NumPy is the fundamental package for scientific computing with Python.\n",
        "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Izy9aiBbw7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uEnphavZQHg"
      },
      "source": [
        "BTW: Line numbers can be enabled by pressing `CTRL`+`M`+`L`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fo1qGrT9_w"
      },
      "source": [
        "## 1 - Obtaining the Image Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8TNYq0uopKY"
      },
      "source": [
        "Execute the code below for downloading the dataset and some utility code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1C-RnD1UtMj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download dataset and `utils`\n",
        "archive = 'Lab1.1.zip'\n",
        "!wget -nv -t 0 --show-progress -O $archive 'https://cloud.tu-ilmenau.de/s/7R3CiEaGdL9KzkB/download'\n",
        "!unzip -j $archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPJtpEXUXWIq"
      },
      "source": [
        "The dataset is provided as archive of numpy arrays, each stored as binary file. We'll unpack them to `X_train, Y_train, X_validation, Y_validation`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4LAeQGhVY76"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "\n",
        "X_train, Y_train, X_validation, Y_validation = utils.load_npz_dataset('binary_flowers.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVkFUskX7s6"
      },
      "source": [
        "Let's plot some images to get an idea about the task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6G_KrfDXjll"
      },
      "outputs": [],
      "source": [
        "utils.plot_samples(X_validation, Y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsrakDD-_Abw"
      },
      "source": [
        "The first row displays images of leaves. We will use them as **negative class** (`y=0`). \n",
        "\n",
        "The second row displays images of flowers. They will constitute our **positive class** (`y=1`). \n",
        "Note that the flowers of weeds are quite hard to distinguish from leaves.\n",
        "\n",
        "**Exercise**: What is the shape of the NumPy arrays `X_validation` and `Y_validation`? How many training samples do you have?\n",
        "\n",
        "**Task**: Print the shape of the arrays `X_validation` and `Y_validation` and state the number of training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVokXOW-5cMz"
      },
      "outputs": [],
      "source": [
        "# GRADED: print the shape of `X_validation` (.5 points)\n",
        "# GRADED: print the shape of `Y_validation` (.5 points)\n",
        "# GRADED: state the number of training samples (.5 points)\n",
        "\n",
        "### START YOUR CODE HERE ### (≈ 3 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsJUxJFEdA4t"
      },
      "source": [
        "### Vectorization over Training Examples\n",
        "\n",
        "In order to vectorize our code, we have to reshape our image arrays of shape `(width, height, channels, num_samples)` to arrays of shape `(num_features, num_samples)`\n",
        "\n",
        "**Task**: Reshape the training and test data. Evaluate your result by plotting the first training sample. For plotting, the utility code reshapes your images. \n",
        "**Make sure the plotted images do not display garbage.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "587Mlhfxc_zG"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: flatten_images (1 point)\n",
        "\n",
        "def flatten_images(X):\n",
        "  \"\"\"Reshape images array `X` to array of shape `(num_features, num_samples)`.\n",
        "\n",
        "  Args:\n",
        "    X (numpy.ndarray): images array, shape = `(width, height, channels, num_samples)`\n",
        "\n",
        "  Returns:\n",
        "    numpy.ndarray: array of shape `(num_features, num_samples)`\n",
        "  \"\"\"\n",
        "  utils.flatten_images.assert_input_shape(X)\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  utils.flatten_images.assert_output_shape(X)\n",
        "  return X\n",
        "\n",
        "\n",
        "# reshape `X_train` to (num_features, num_samples)\n",
        "X_train = flatten_images(X_train)\n",
        "utils.plot_sample(X_train)\n",
        "\n",
        "# reshape `X_validation` to (num_features, num_samples)\n",
        "X_validation = flatten_images(X_validation)\n",
        "utils.plot_sample(X_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kER-FRHybzWP"
      },
      "source": [
        "## 2 - Implement Layers and Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qczOKa1oA__j"
      },
      "source": [
        "Our network will contain dense layers and sigmoid activation layers. We objectify these layers for making the forward and backward computations easier. In detail, each layer class will have a `forward` and `backward` method that are callable on arbitrary inputs of defined shape.\n",
        "\n",
        "**Task**: Complete the `forward`, `backward`, and `update` methods of the `Dense` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQVAkgS_SjAF"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: Dense.forward (1 point)\n",
        "# GRADED FUNCTION: Dense.backward (1 point)\n",
        "# GRADED FUNCTION: Dense.update (1 point)\n",
        "\n",
        "class Dense:\n",
        "  \"\"\" Dense layer with weight matrix `W` and bias vector `b`.\n",
        "  \n",
        "  Attributes:\n",
        "  ---\n",
        "    num_neurons (int): number of neurons (equals the output size)\n",
        "    input_shape (tuple): shape of the input data\n",
        "    W (numpy.ndarray): layer weights\n",
        "    b (numpy.ndarray): layer bias\n",
        "    input (numpy.ndarray): layer input\n",
        "    output (numpy.ndarray): layer output\n",
        "    dW (numpy.ndarray): derivative w.r.t. weights `W`\n",
        "    db (numpy.ndarray): derivative w.r.t. bias `b`\n",
        "    gradient_out (numpy.ndarray): derivative w.r.t. layer input `input`\n",
        "  \n",
        "  Methods:\n",
        "  ---\n",
        "    forward( input )\n",
        "    backward( gradient_in )\n",
        "    update()\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_neurons, input_shape):\n",
        "    \"\"\"Init a dense layer\n",
        "\n",
        "    Initialize a dense layer with number of neurons = `neurons` for an input of \n",
        "    shape `input_shape`.\n",
        "    Weights `W` are randomly initialized from normal distribution. Bias `b` is \n",
        "    initialized as zeros.\n",
        "\n",
        "    Args:\n",
        "        num_neurons (int): number of neurons in layer; equal to output dimension of layer\n",
        "        input_shape (tuple): shape of input data `(num_features, num_samples)`\n",
        "    \"\"\"\n",
        "\n",
        "    self.input_shape = input_shape\n",
        "    self.num_neurons = num_neurons\n",
        "\n",
        "    # initialize with normal distributed random weights\n",
        "    self.W = np.random.normal( scale=.1, size=(self.num_neurons, self.input_shape[0]))\n",
        "    self.b = np.zeros( (self.num_neurons, 1) )\n",
        "\n",
        "    # allocate output (neurons, m)\n",
        "    self.output = np.zeros( (self.num_neurons, self.input_shape[1]))\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    \"\"\" Forward pass of `input` through dense layer. Output is stored in `output`.\n",
        "\n",
        "    Args:\n",
        "      input (numpy.ndarray): layer input\n",
        "    \"\"\"\n",
        "    \n",
        "    # store layer input\n",
        "    self.input = input\n",
        "    \n",
        "    # compute layer output\n",
        "    ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "  def backward(self, gradient_in):\n",
        "    \"\"\" Backward pass of `gradient_in` through dense layer. \n",
        "    Computes derivatives (gradients) w.r.t. weights, bias, and layer input.\n",
        "    Gradients are stored in `dW`, `db`, `gradient_out`.\n",
        "\n",
        "    Args:\n",
        "      gradient_in (numpy.ndarray): derivative w.r.t. layer output\n",
        "    \"\"\"\n",
        "\n",
        "    # derivative w.r.t. `W`\n",
        "    ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    # derivative w.r.t. `b`\n",
        "    self.db = np.sum(gradient_in, axis=1, keepdims=True)\n",
        "\n",
        "    # derivative w.r.t. layer input\n",
        "    self.gradient_out = np.dot(self.W.T, gradient_in)\n",
        "\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\" Update layer parameters `W`, `b` using global `LEARNING_RATE` \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (≈ 2 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "  @property\n",
        "  def number_of_parameters(self):\n",
        "    if not hasattr(self, '_number_of_parameters'):\n",
        "      self._number_of_parameters = np.sum(\n",
        "          [getattr(self, parameter_type).size for parameter_type in ('W', 'b')]\n",
        "      )\n",
        "    return self._number_of_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as for the `Dense` layer, we will implement the activation function as a **layer**, also with a `forward` and a `backward` method.\n",
        "\n",
        "**Task**: Complete the class `Sigmoid` by implementing the `forward` and `backward` method."
      ],
      "metadata": {
        "id": "CiGX4EmxHTrA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N80wJ9BEq5Ql"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: Sigmoid.forward (1 point)\n",
        "# GRADED FUNCTION: Sigmoid.backward (1 point)\n",
        "\n",
        "class Sigmoid:\n",
        "  \"\"\" Activation layer using the logistic function (aka sigmoid function)\n",
        "  \n",
        "  Attributes:\n",
        "  ---\n",
        "    output (numpy.ndarray): layer output\n",
        "    gradient_out (numpy.ndarray): derivative w.r.t. layer input `Z`\n",
        "  \n",
        "  Methods:\n",
        "  ---\n",
        "    forward( input )\n",
        "    backward( gradient_in )\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_shape):\n",
        "    \"\"\"Init an activation layer using the Sigmoid activation function.\n",
        "\n",
        "    Initialize an activation layer for element-wise calculation of the sigmoid\n",
        "    of a given input of shape `input_shape`.\n",
        "\n",
        "    Args:\n",
        "      input_shape (tuple): shape of input data `(num_features, num_samples)`\n",
        "    \"\"\"\n",
        "\n",
        "    # allocate output\n",
        "    self.output = np.zeros( input_shape )\n",
        "\n",
        "  def forward(self, input):\n",
        "    \"\"\"Forward pass of `input` through sigmoid layer. \n",
        "    \n",
        "    Compute the value of the sigmoid function for each element of `input`.\n",
        "    The output is stored in `output`.\n",
        "\n",
        "    Args:\n",
        "        input (numpy.ndarray): layer input\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "  def backward(self, gradient_in):\n",
        "    \"\"\"Backward pass of `gradient_in` through sigmoid layer. \n",
        "    The output is stored in `gradient_out`.\n",
        "\n",
        "    Args:\n",
        "        gradient (numpy.ndarray): gradient w.r.t. to layer input\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At last, we'll need a loss function to compute the mismatch between model prediction and ground truth. As we are solving a binary classification task, we'll use the **binary cross-entropy loss** defined for one sample as\n",
        "\n",
        "$l(y, \\hat{y}) = -y \\log (\\hat{y}) - (1-y) \\log (1-\\hat{y})$.\n",
        "\n",
        "To assess our model's performance, the function `binary_cross_entropy_loss` shall return the **average loss** across all $m_\\mathrm{batch}$ samples in a minibatch:\n",
        "\n",
        "$\\mathcal{L} = \\frac{1}{m_\\mathrm{batch}}\\sum_{i=1}^{m_\\mathrm{batch}} l(y, \\hat{y})$.\n",
        "\n",
        "\n",
        "**Task**: Implement the binary cross-entropy loss `loss` and its derivative w.r.t. the model predictions `dY_hat`."
      ],
      "metadata": {
        "id": "yHA714J6KzWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz1PRMRqscom"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: binary_cross_entropy_loss (2 points)\n",
        "\n",
        "def binary_cross_entropy_loss(Y, Y_hat):\n",
        "  \"\"\"Compute binary cross-entropy loss across predictions `Y_hat` and its gradient w.r.t. `Y_hat`.\n",
        "\n",
        "  Args:\n",
        "    Y (numpy.ndarray): Ground truth data labels (expected output)\n",
        "    Y_hat (numpy.ndarray): Predicted labels (output of last layer)\n",
        "\n",
        "  Returns:\n",
        "    loss (numpy.float64) : average value of the binary cross-entropy loss\n",
        "    dY_hat (numpy.ndarray): gradient of loss w.r.t. Y_hat\n",
        "  \"\"\"\n",
        "  # prevent log(0)\n",
        "  Y_hat += 1e-9\n",
        "\n",
        "  ### START YOUR CODE HERE ###  (≈2 LOC)\n",
        "  loss = \n",
        "  dY_hat =\n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  return loss, dY_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woVkBjxQeW5x"
      },
      "source": [
        "## 3 - Implementing a 2 Layer Neural Network\n",
        "\n",
        "Using the layers and the loss function defined above, we can now implement our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDNFuGtQeVgU"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: NeuralNet.forward (4 points)\n",
        "# GRADED FUNCTION: NeuralNet.backward (4 points)\n",
        "# GRADED FUNCTION: NeuralNet.update (2 point)\n",
        "\n",
        "class NeuralNet(utils.NeuralNet_base):\n",
        "  \"\"\"Your feed-forward neural network.\n",
        "  \n",
        "  Attributes:\n",
        "  ---\n",
        "    input_shape (tuple): shape of input data\n",
        "    loss_function (func): loss function to quantify the network's prediction error\n",
        "    batch_size (int): batch size (default=100)\n",
        "    number_of_parameters (int): number of trainable model parameters\n",
        "  \n",
        "  Methods:\n",
        "  ---\n",
        "    prepare_layers()\n",
        "    forward()\n",
        "    backprop()\n",
        "    update()\n",
        "    fit()\n",
        "    predict()\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_shape, loss_function, *args, batch_size=100, shuffle=True, **kwargs):\n",
        "    \"\"\"Initialize neural network with architecture defined in `prepare_layers`\n",
        "\n",
        "    Initialize a neural network using the architecture defined in function\n",
        "    `prepare_layers`\n",
        "\n",
        "    Args:\n",
        "        loss_function (func): loss function to quantify the network's prediction error\n",
        "        input_shape (tuple): shape of input data `(num_features, num_samples)`\n",
        "        batch_size (int): number of samples in minibatch (default=100)\n",
        "        shuffle (bool): shuffle training data in each epoch (default=True)\n",
        "    \"\"\"\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.input_shape = input_shape\n",
        "    self.loss_function = loss_function\n",
        "    self.batch_size = batch_size\n",
        "    self.prepare_layers()\n",
        "\n",
        " \n",
        "  def prepare_layers(self):\n",
        "    \"\"\"Initialize layers to be used in the neural network\"\"\"\n",
        "\n",
        "    # hidden layer with 10 neurons\n",
        "    self.hidden_layer = Dense(10, (self.input_shape[0], self.batch_size))\n",
        "    # sigmoid activation of hidden layer output\n",
        "    self.hidden_layer_activation = Sigmoid(self.hidden_layer.output.shape)\n",
        "    \n",
        "    # output layer with 1 neuron (binary classification)\n",
        "    self.output_layer = Dense(1, self.hidden_layer_activation.output.shape)\n",
        "    # sigmoid activation of output layer output\n",
        "    self.output_layer_activation = Sigmoid(self.output_layer.output.shape)\n",
        "\n",
        "    print('Initialized Neural Network with {} parameters.'.format(self.number_of_parameters))\n",
        "\n",
        "  \n",
        "  def forward(self, X):\n",
        "    \"\"\"Forward propagate input `X` through network and return network output.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): input data of shape `(num_features, num_samples)`\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: output of last layer (\"Y_hat\")\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ### (≈4 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    return self.output_layer_activation.output # (= \"Y_hat\")\n",
        "\n",
        "  def backprop(self, dY_hat):\n",
        "    \"\"\"Backpropagate gradient `dY_hat` w.r.t. predicted output through the network. \n",
        "\n",
        "    Args:\n",
        "        dY_hat (numpy.ndarray): gradient w.r.t. predicted output\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ### (≈4 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"Call `update` method of each trainable layer\"\"\"\n",
        "    ### START YOUR CODE HERE ### (≈2 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BpQnPBRxQxv"
      },
      "source": [
        "## 4 - Testing the Forward Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLohfYiG0Qoz"
      },
      "source": [
        "Now it's time to initialize the network and test the forward propagation.\n",
        "\n",
        "Let's define some hyperparameters and create an instance of our `NeuralNet`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OdF32RKx76j"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCHSIZE = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHYxu7pKeVjY"
      },
      "outputs": [],
      "source": [
        "my_NN = NeuralNet(X_train.shape, binary_cross_entropy_loss, batch_size=BATCHSIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bku44AHNyqmd"
      },
      "source": [
        "The layers were initialized with random weights. Hence, the untrained network will return random predictions.\n",
        "The validation split is balanced, i.e., it contains the same amount of positive and negative samples. For binary classification on balanced data, random guessing should return an accuracy of roughly 50%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trUrRfbuQJo4"
      },
      "outputs": [],
      "source": [
        "Y_validation_hat = my_NN.predict( X_validation )\n",
        "print('Accuracy: {:.1f}%'.format(100*np.mean( Y_validation_hat == Y_validation )))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5RWBFL-z4mT"
      },
      "source": [
        "## 5 - Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIiy0_k8z-g5"
      },
      "source": [
        "Now it's time to finally train your network. \n",
        "\n",
        "Train it for 100 epochs with a minibatch size of 100 using the `.fit()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-akm6zPexq-B"
      },
      "outputs": [],
      "source": [
        "history = my_NN.fit(X_train, Y_train, X_validation, Y_validation, NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26J5VtWg1KkC"
      },
      "source": [
        "We can again call the `.predict()` method on the validation data again and see how the network improved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUIkhYKE1Dqg"
      },
      "outputs": [],
      "source": [
        "Y_validation_hat = my_NN.predict( X_validation )\n",
        "print('Accuracy: {:.1f}%'.format(100*np.mean( Y_validation_hat == Y_validation )))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmrnju_G1mUm"
      },
      "source": [
        "In addition, we stored the loss as well the training and validation accuracy of the network after each epoch in the `history` dictionary. Let's plot the learning curve, i.e., the loss over epochs, and the accuracies:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.plot_history(history)"
      ],
      "metadata": {
        "id": "Dmw_5BwAHE_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOUx8N842EMN"
      },
      "source": [
        "---\n",
        "\n",
        "# Congratulations!\n",
        "\n",
        "You've just coded your own neural network in NumPy and trained it to recognize flowers in images. If everything went smooth, you should achieve a training accuracy of > 95% and validation accuracy > 80%.\n",
        "You may play a little with the hyperparameters, e.g., change the `LEARNING_RATE` and `BATCHSIZE`. In addition, you may add more parameters to the model by increasing the hidden layer size, i.e., the amount of neurons of the hidden layer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}