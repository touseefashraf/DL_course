{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/touseefashraf/DL_course/blob/main/DL_Lab_2_2_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_9de7rQ0zLH"
      },
      "source": [
        "# DL Lab 2.2 - Homework - Transfer Learning and Fine-Tuning of Pretrained Models\n",
        "\n",
        "In the last lab, you built a simple ConvNet, trained it from scratch, added augmentation and regularization techniques, and achieved about 90% accuracy.\n",
        "\n",
        "In this homework, you will use two techniques for (re-)using the \"experience\" stored in **pre-trained** models. These techniques are extremely useful if you don't have suffcient data for training a bigger model from scratch. Data such as our images of 10 different classes of flowers. ;-)\n",
        "\n",
        "In detail, you will use **transfer learning** and **fine-tuning** of a model that was already trained on a very large dataset, namely the *ImageNet ILSVRC* data.\n",
        "\n",
        "In the end, you will investigate how our ConvNet actually perceives and recognizes the given data by **visualizing feature maps** as well performing **Activation Maximization**.\n",
        "\n",
        "***\n",
        "\n",
        "**After completing this homework you will be able to**\n",
        "\n",
        "- Use **pretrained models** for **transfer-learning** and **fine-tuning**\n",
        "- alter the **train scope** of specific layers of your models\n",
        "- **interprete** what your model has learned.\n",
        "\n",
        "***\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- You'll be using Python 3 in the iPython based Google Colaboratory\n",
        "- Lines marked by \"<font color='green'>`# TODO`</font>\" denote the code fragments to be completed by you.\n",
        "- There's no need to write any other code.\n",
        "- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n",
        "- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" in the \"<font color='green'>`# TODO`</font>\" comments to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
        "- If you get stuck, check your Lecture and Lab notes and use the [discussion forum](https://moodle2.tu-ilmenau.de/mod/forum/view.php?id=166458) in Moodle.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnLdskLJKVS"
      },
      "source": [
        "**Note**: Training a ConvNet is a computationally expensive process. Most of the computations can be parallelized very efficently, making them a perfect fit for GPU-acceleration. In order to enable a GPU for your Colab session, do the following steps:\n",
        "- Click '*Runtime*' -> '*Change runtime type*'\n",
        "- In the pop-up window for '*Hardware accelerator*', select '*GPU*' \n",
        "- Click '*Save*'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__7Oh3RUSUCI"
      },
      "source": [
        "# 0 - Test for GPU\n",
        "\n",
        "Execute the code below for printing the TF version and testing for GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3i2rd_teRGJv"
      },
      "outputs": [],
      "source": [
        "#@title Print TF version and GPU stats\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFyNVcbwj959"
      },
      "source": [
        "# 1 - Download and Prepare the Data\n",
        "Execute the cells below to download the data and configure the data generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F2O82Brfj8Ih"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Downloader\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#@title Dataset Downloader\n",
        "DEST_PATH = '/tmp/flowers10.zip'\n",
        "\n",
        "!wget -nv -t 0 --show-progress -O $DEST_PATH 'https://cloud.tu-ilmenau.de/s/WGNk32LRQ847rS6/download/flowers10.zip'\n",
        "!sleep 1\n",
        "!unzip -uq $DEST_PATH\n",
        "!rm  $DEST_PATH\n",
        "\n",
        "base_dir = 'flowers10/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2BqXU53bOTXR"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Data Generators\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.4,\n",
        "    brightness_range=(.5, 1.5),\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "# Flow validation images in batches using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_steps = np.ceil(train_generator.samples / train_generator.batch_size)  # 800 images = batch_size * steps\n",
        "val_steps = np.ceil(validation_generator.samples / validation_generator.batch_size)  # 200 images = batch_size * steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jbua08yz7Rc"
      },
      "source": [
        "# 2 - Transfer-Learning Using Pretrained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbzQ41pq16_T"
      },
      "source": [
        "## 2.1 - Feature Extraction for Downstream Classification\n",
        "\n",
        "One thing that is typically done in computer vision tasks is to take a model trained on a very large dataset, e.g., the *ImageNet ILSVRC* data, and use this model for feature extraction on smaller datasets. Even though the dataset and task the model was originally trained on might be quite different to the actual dataset and task, the extracted features are typically very informative. This versatility and repurposability of ConvNets is one of the most interesting aspects of Deep Learning. The extracted features (aka **representations**) can then be used for downstream tasks such as classification.\n",
        "\n",
        "In this homework, you will use the famous [VGG16 model](https://arxiv.org/abs/1409.1556) named after the **V**isual **G**eometry **G**roup from Oxford. The model was pre-trained on the ImageNet ILSVRC data, i.e., a large dataset of web images (1.4M images and 1000 classes). Although outperformed by more recent architectures, this simple but powerful model won the ILSVR challenge in 2014 and remains very famous for feature extraction since then. Let's see how these features help for our flower classification task!\n",
        "\n",
        "First, we need to pick which intermediate layer of VGG16 we will use for feature extraction. A common practice is to use the output of the last convolution layer before the fully connected (aka dense) layers. The fully connected layers are too specialized for the original task the network was trained in. Hence using their outputs as features won't be very useful for a new task.\n",
        "\n",
        "The zoo of pre-trained models is provided in the [`keras.applications` module](https://www.tensorflow.org/api_docs/python/tf/keras/applications) in TF. You can directly load the architecture from there. If you set `weights='imagenet'`, the `imagenet` weights will be loaded automatically.\n",
        "\n",
        "If `include_top=False` is specified, the network is loaded without the original classification layers at the top. Instead the network's last layer is a `MaxPooling2D` layer pooling across the output of the last convolution layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHjNGgp3203"
      },
      "source": [
        "**Task**: Complete the code below for building a feature extraction model based on the VGG16. Use a `GlobalMaxPooling2D` layer on the output of the pre-trained model.\n",
        "\n",
        "**Hint**: Layers are callable objects. Same as models, they have the properties `.input` and `.output`, providing their input and output tensors.\n",
        "\n",
        "**Hint$^2$**: `pre_trained_model.input` provides the input tensor of the pretrained VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYkfle4RLpUo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def build_feature_extraction_model(input_shape, summary=True):\n",
        "\n",
        "  ### START YOUR CODE HERE ###  (≈3 LOC)\n",
        "\n",
        "  # Initialize the pre-trained model\n",
        "  pre_trained_model = \n",
        "  \n",
        "  # Add global maxpooling layer on output of pre-trained model\n",
        "\n",
        "  # Define the feature_extraction_model\n",
        "  feature_extraction_model = \n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  # Freeze pre-trained model\n",
        "  pre_trained_model.trainable = False\n",
        "\n",
        "  if summary:\n",
        "    print(feature_extraction_model.summary())\n",
        "\n",
        "  return feature_extraction_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaLRaZRINODZ"
      },
      "outputs": [],
      "source": [
        "feature_extraction_model = build_feature_extraction_model( (150,150,3) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0avWf142l-US"
      },
      "source": [
        "You can now use the `feature_extraction_model` for computing powerful representations of the images in your dataset. We create a utility function that reads the data from the data generators in batches and accumulates the extracted features and their associated class labels in the memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuyC-xFTNORH"
      },
      "outputs": [],
      "source": [
        "def extract_features_in_batches(model, generator, repetitions=1):\n",
        "  ''' Loop over `generator` batches and return extracted features and labels '''\n",
        "\n",
        "  X, Y = ( list(), list() )\n",
        "  generator.reset()\n",
        "  \n",
        "  for i in range(repetitions):\n",
        "\n",
        "    if i:\n",
        "      print('\\nrepetition', i)\n",
        "\n",
        "    batch_index = 0\n",
        "    while batch_index <= generator.batch_index:\n",
        "        sys.stderr.write('\\rbatch {}'.format(batch_index) )\n",
        "        batch_x, batch_y = generator.next()\n",
        "        Y.extend( batch_y )\n",
        "        X.extend( model.predict( batch_x ) )\n",
        "        batch_index += 1\n",
        "  \n",
        "  return np.asarray(X), np.asarray(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbPNr-KulPEV"
      },
      "source": [
        "**Task**: Extract validation and training features (and their associated labels) using the `feature_extraction_model` on the `validation_generator` and the `training_generator`. For **augmenting** the training data, use **five repetitions** on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYUCIOEINOUi"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ###  (≈2 LOC)\n",
        "\n",
        "# Validation features `X_val` and labels `Y_val`\n",
        "X_val, Y_val = \n",
        "\n",
        "# Training features `X_train` and labels `Y_train`\n",
        "X_train, Y_train = \n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50-EMd0tns0f"
      },
      "source": [
        "Next, build a small Neural Network consisting of two hidden dense layers of 512 neurons each and 20% dropout, followed by a dense layer for classification. Don't forget to make reasonable decisions about the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI3PahArNOaB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_classifier_model(input_shape, num_classes, init_lr=1e-3):\n",
        "    ### START YOUR CODE HERE ### (≈7 LOC)\n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=init_lr),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "  \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p864yInEI5En"
      },
      "outputs": [],
      "source": [
        "#@title `plot_history()` definition\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n",
        "  ax1.plot(history.history['loss'], label='training')\n",
        "  ax1.plot(history.history['val_loss'], label='validation')\n",
        "  ax1.set_ylabel('Cross-Entropy Loss')\n",
        "  ax1.set_yscale('log')\n",
        "  if history.history.__contains__('lr'):\n",
        "    ax1b = ax1.twinx()\n",
        "    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n",
        "    ax1b.set_yscale('log')\n",
        "    ax1b.set_ylabel('Learning Rate', color='g')\n",
        "\n",
        "  ax2.plot(history.history['accuracy'], label='training')\n",
        "  ax2.plot(history.history['val_accuracy'], label='validation')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.legend()\n",
        "  plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kq95C-Oo90Y"
      },
      "source": [
        "Using the extracted features, you can now train your classification model. Let's see how it performs after 50 epochs of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b9eM145ZmnJ"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model( X_val.shape[1], Y_val.shape[1], init_lr=1e-4)\n",
        "\n",
        "history = classifier_model.fit(\n",
        "    x=X_train,\n",
        "    y=Y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    verbose=2\n",
        ")\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqbOJ2uFpdoT"
      },
      "source": [
        "You should make two observations:\n",
        "\n",
        "1.   Without much effort, you achieve roughly the same validation accuracy as with the simple ConvNet designed in the last lab. :-D\n",
        "2.   The model is converging quite fast but is plateauing on the validation data. Remember that every training image was only augmented five times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHiMsmK8RWmU"
      },
      "source": [
        "## 2.2 - Learning new Heads on pre-trained Models\n",
        "\n",
        "Instead of a) using the pre-trained ConvNet as feature extractor and b) another classifier model, you can do both jobs a) + b) in one and the same model. For this, you simply add your classifier on top of your ConvNet backbone (or replace the \"old\" classifier by your \"new\" classifier).\n",
        "\n",
        "**Task**: Complete the code below in order to add a classifier on top of the VGG16 stub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y99WKS9YiZkR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def build_model(input_shape, num_classes, summary=True):\n",
        "\n",
        "    pre_trained_model = VGG16(\n",
        "        input_shape=input_shape,\n",
        "        weights='imagenet',\n",
        "        include_top=False\n",
        "    )\n",
        "\n",
        "    ### START YOUR CODE HERE ### (≈7 LOC)\n",
        "    \n",
        "    # Add global maxpooling layer on output of pre-trained model\n",
        "    \n",
        "    # Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "    \n",
        "    # Add a dropout layer with rate 0.2\n",
        "    \n",
        "    # Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "    \n",
        "    # Add a dropout layer with rate 0.2\n",
        "    \n",
        "    # Add a final softmax layer for classification\n",
        "    \n",
        "    # Define the model\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    if summary:\n",
        "        print(model.summary())\n",
        "\n",
        "    return model, pre_trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfgN6XWgLe-q"
      },
      "outputs": [],
      "source": [
        "model, pre_trained_model = build_model((150,150,3), num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl9m4eHYk4nm"
      },
      "source": [
        "You want to use the pretrained VGG16 backbone only for feature extraction. In order to prevent any weights of the pretrained model from beeing updated during training, you will **freeze** it. Only the new layers on top shall be trainable.\n",
        "\n",
        "In Keras, you can easily access the list of layers of a model using its `layers` property. Each layer has a `trainable` property defining whether or not to update it's weights during training. You can set `trainable = False` for all layers of the pretrained VGG16 model. \n",
        "\n",
        "Instead of setting the `trainable` property per layer, you can also set the `trainable` property of entire models!\n",
        "\n",
        "Anyway, the model summary should tell you the reduced number of trainable parameters, amounting to ≈530k parameters. Also compare the total number of parameters: the simple ConvNet designed in last lab's homework assignment roughly had ≈24k parameters. The VGG16 based model contains ≈15M parameters!\n",
        "\n",
        "**Task**: Freeze all weights of the `pre_trained_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gx_s13JlORf"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ### (≈1 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7imMTOYlR8P"
      },
      "source": [
        "Whenever you change the `trainable` setting of any layer, you need to compile your model for making the changes take effect.\n",
        "\n",
        "**Task**: Compile the model. For compilation, use the \n",
        "`'categorical_crossentropy'` as loss function, Adam optimizer with learning rate `INITIAL_LEARNING_RATE`, and `'accuracy'` as metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89PbHLi_iZyJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "INITIAL_LEARNING_RATE = 1e-3\n",
        "\n",
        "### START YOUR CODE HERE ### (≈1 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtHcSIcDl6Oy"
      },
      "source": [
        "Train the model for 30 epochs and check the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "8jDrky4niaKg"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "plot_history( history )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVejBTyb6bgk"
      },
      "source": [
        "Despite the smaller number of samples per epoch, the time for each epoch is notably larger as every image needs to be loaded from disk and propagated through the entire ConvNet. On the other hand, every image is randomly augmented in every epoch, which should increase the model's generalizability. Training for more epochs while using learning rate decay would likely improve the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Y7ZcFi1PIV"
      },
      "source": [
        "# 3 - Fine-Tuning Pre-trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmDB2kJXrBLA"
      },
      "source": [
        "## 3.1 - Unfreeze Layers\n",
        "In the previous two experiment, you only trained the weights of the new layers, both as independent model as well as in the joint model for feature extraction and classification. The weights of the pre-trained network were not updated during training. \n",
        "\n",
        "To increase the accuracy, you can also **fine-tune** the weights of the pre-trained network for learning more discriminant representations of your data. All you need to to is **unfreeze** the layers of the pre-trained network. Note that you have to recompile the model in order to let this take effect. In comparison to the learning rate used during transfer learning, the learning rate used for fine-tuning is typically smaller. \n",
        "\n",
        "**Task**: Unfreeze the layers of the pre-trained model, i.e., make them trainable. Then compile the model again and compare with the model summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuxK_TvZs2-J"
      },
      "outputs": [],
      "source": [
        "INITIAL_LEARNING_RATE = 3e-5\n",
        "\n",
        "### START YOUR CODE HERE ### (≈2 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQiBC0svYULU"
      },
      "source": [
        "## 3.2 - Using Callbacks\n",
        "\n",
        "You will define [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) for scheduling the learning rate as training progresses. \n",
        "Callbacks are utility classes that are called in every epoch during training. In the cell below, the custom function `lr_step_decay` defines the step-wise reduction of the learning rate by 90% after every ten epochs. The \n",
        "[Learning rate scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) callback `LRDecayCallback` then uses this function for actually setting the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX8xlqptW3yg"
      },
      "outputs": [],
      "source": [
        "def lr_step_decay(epoch, lr, drop=.9, drop_epochs=10):\n",
        "    if epoch < 10:\n",
        "        return INITIAL_LEARNING_RATE\n",
        "    else:\n",
        "        return INITIAL_LEARNING_RATE * np.power(drop, np.floor(epoch/drop_epochs))\n",
        "\n",
        "LRDecayCallback = tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9T1D4BTXse8"
      },
      "source": [
        "In addition to the learning rate callback, you will also use an [Early stopping callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) that stops the training process as soon as the validation loss stops decreasing. You will use this `StopCallback` for preventing your model from overfitting on the training data.\n",
        "\n",
        "**Task**: Create an early stopping callback monitoring the validation loss and stopping the training if the validation loss did not increase for ten epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-9rS9WEXX1F"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "StopCallback = \n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIcFnQPRss0Y"
      },
      "source": [
        "Now, continue to train the model for a maximum of 200 epochs. Please note the VGG16 model is really large and quite slow during training.\n",
        "\n",
        "**Task**: Add the `LRDecayCallback` and `StopCallback` to the call of the `fit` method and fine-tune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmoW4w8ItGfZ"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=200,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps,\n",
        "    verbose=2,\n",
        ")\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "plot_history( history )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dZFpag1stGI"
      },
      "source": [
        "**Notes on fine-tuning**:\n",
        "\n",
        "- Fine-tuning should only be attempted *after* you have trained the new head layers with the frozen pre-trained model.\n",
        "- Depending on the amount of training data available, you might fine-tune only *few layers* of the pre-trained model rather than all layers of the pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9aVjZDju0jk"
      },
      "source": [
        "# 4 - Interpreting ConvNets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPrQCl-d_6Ed"
      },
      "source": [
        "## 4.1 - Visualizing Feature Maps\n",
        "\n",
        "Feature maps display the output of a convolution layer after forwarding the input data through the network. The idea of visualizing a feature map for a specific input image would be to understand what features of the input are activated in a feature map. We expect feature maps close to the input to activate small details, such as corners, edges, as well as colors, whereas feature maps close to the output of the model capture more abstract and high-level concepts.\n",
        "\n",
        "First, we need a clearer idea about the output shape and layer index of the convolution layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "162nolIOx4HE"
      },
      "outputs": [],
      "source": [
        "for layer_idx, layer in enumerate(model.layers):\n",
        "    # check if convolutional layer\n",
        "    if not 'convolutional' in str(layer.__class__):\n",
        "        continue\n",
        "    print(layer_idx, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OniKaX64zfoh"
      },
      "source": [
        "You can now use this information for designing a new model that contains a subset of the layers from the full VGG16 model. The model would have similar input layer as your original model, but the output would be the output of a given convolutional layer.\n",
        "\n",
        "**Task**: Create a new model `visualization_model` that returns the output of the first convolutional layer (`layer_idx=1`):\n",
        "\n",
        "**Hint**: Use the `layers` property of the model and the `output` property of the respective layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4TuQ0HPzTqH"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "visualization_model = \n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "print(visualization_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4BZK7kw0Zdw"
      },
      "source": [
        "Let's pick a random image from your validation data for exploring the feature map visualizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G7wD_mczp4n"
      },
      "outputs": [],
      "source": [
        "def show(img):\n",
        "    '''display image'''\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.grid(False)\n",
        "    plt.axis('Off')\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "img = validation_generator.next()[0][0]\n",
        "show(img)\n",
        "\n",
        "# expand dimensions so that it fakes a batch containing a single sample\n",
        "img = np.expand_dims(img, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewj6mtij1K0i"
      },
      "source": [
        "You are now ready to get the feature maps by forwarding the image through the network calling `visualization_model.predict()`.\n",
        "\n",
        "The result will be feature maps with shape `(150, 150, 64)`. Let's plot the result as an 8x8 array of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogBwuB4r0UCT"
      },
      "outputs": [],
      "source": [
        "feature_maps = visualization_model.predict(img) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "27bpVRQS6A5o"
      },
      "outputs": [],
      "source": [
        "#@title Plot feature maps\n",
        "square = 8\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(square*2,square*2)\n",
        "idx = 1\n",
        "for _ in range(square):\n",
        "    for _ in range(square):\n",
        "        sp = plt.subplot(square, square, idx)\n",
        "        sp.axis('Off')\n",
        "        sp.title.set_text(str(idx-1))\n",
        "        plt.imshow(feature_maps[0, :, :, idx-1])\n",
        "        idx += 1\n",
        "\n",
        "plt.show()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIZBM-yj3y6k"
      },
      "source": [
        "You can see that the result of applying the filters of the first convolution layer is a lot of versions of the original flower image with different features highlighted.\n",
        "\n",
        "In order to print feature maps of deeper convolution layers, you simply update the `visualization_model` for the index of the targeted layer. You can also collect feature maps from each block of the model in a single forward pass, and then create a square of images for each block.\n",
        "\n",
        "In the VGG16 architecture, there are five main blocks `block1`, `block2`, and so on. The layer_indices of the first convolutional layer in each block are `[1, 4, 7, 11, 15]`.\n",
        "\n",
        "**Task**: Define a new `visualization_model` that returns the outputs of the first convolutional layers in each block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl_KOtF50zvJ"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ### (≈2 LOC)\n",
        "layer_indices = \n",
        "visualization_model = \n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3JARpmB4ICz"
      },
      "source": [
        "For sake of visibility, let us cap the number of displayed feature maps per layer at 16:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yh_tqSUQ4DJK"
      },
      "outputs": [],
      "source": [
        "#@title Plot feature maps\n",
        "plt.imshow(img[0])\n",
        "plt.axis('Off')\n",
        "plt.title('Input Image')\n",
        "plt.show()\n",
        "\n",
        "square = 8\n",
        "feature_maps = visualization_model.predict(img)\n",
        "for layer_idx, fmap in enumerate(feature_maps):\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(square*2,square*2)\n",
        "  fig.suptitle( model.layers[ layer_indices[layer_idx] ].name )\n",
        "  idx = 0\n",
        "  for _ in range(square):\n",
        "    for _ in range(2):\n",
        "      fm = fmap[0, :, :, idx]\n",
        "      sp = plt.subplot(square, square, idx+1)\n",
        "      sp.axis('Off')\n",
        "      sp.title.set_text(str(idx))\n",
        "      plt.imshow(fm)\n",
        "      idx += 1\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p06vHfK05gFx"
      },
      "source": [
        "As expected, the feature maps closer to the input capture a lot of fine details in the image. As you progress deeper into the model, the feature maps show less and less details. Some channels show large activations at the original location of the flower in the image.\n",
        "\n",
        "Analyzing all channels would be rather time-consuming. Instead, let's visualize where your model *looks* on average, i.e., the average of the feature maps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GyfIf3vI8FFX"
      },
      "outputs": [],
      "source": [
        "#@title Plot average feature maps\n",
        "plt.imshow(img[0])\n",
        "plt.axis('Off')\n",
        "plt.title('Input Image')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure(figsize=(14, 14))\n",
        "for layer_idx, fmap in enumerate(feature_maps):\n",
        "  sp = fig.add_subplot(1, len(feature_maps), layer_idx+1)\n",
        "  sp.axis('Off')\n",
        "  sp.title.set_text(model.layers[ layer_indices[layer_idx] ].name)\n",
        "  plt.imshow(np.squeeze(fmap.mean(axis=-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1M-cWdSVOKx"
      },
      "source": [
        "Instead of plotting the average of the feature maps, it can be quite informative to visualize the feature map that had the largest overall activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h1TAB6NtUOIC"
      },
      "outputs": [],
      "source": [
        "#@title Plot max activation feature maps\n",
        "plt.imshow(img[0])\n",
        "plt.axis('Off')\n",
        "plt.title('Input Image')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure(figsize=(14, 14))\n",
        "for layer_idx, fmap in enumerate(feature_maps):\n",
        "  sp = fig.add_subplot(1, len(feature_maps), layer_idx+1)\n",
        "  sp.axis('Off')\n",
        "  sp.title.set_text(model.layers[ layer_indices[layer_idx] ].name)\n",
        "  max_idx = np.argmax(np.sum(fmap, axis=(1,2)), axis=-1)\n",
        "  plt.imshow(np.squeeze(fmap[:,:,:,max_idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MItS2fVa5r4R"
      },
      "source": [
        "## 4.2 - Visualizing Convolution Filters\n",
        "\n",
        "Now that you learned how to analyze where your network *looks*, let us visualize how your network actually perceives images. In detail, you will visualize input patterns that activate specific filters of your network by **activation maximization**.\n",
        "\n",
        "The idea is to optimize the pixel values of a random input image via **gradient ascent** in order to maximize the average of a specific feature map.\n",
        "\n",
        "For performing gradient ascent, you need to:\n",
        "1. Freeze the network (you don't want to update any weights of the network).\n",
        "2. Forward a randomly initialized image through the network.\n",
        "3. Compute the average of a specific feature map in a layer.\n",
        "4. Compute the gradient of the average feature map with respect to the pixel values of the input image.\n",
        "5. Update the pixel values based on the backpropagated gradient.\n",
        "6. Repeat steps 2-5\n",
        "\n",
        "Make it so!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFks08Q6wWr"
      },
      "source": [
        "**Task**: Freeze all layers of `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMD2MwblBMMU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "### START YOUR CODE HERE ### (≈2 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA0_n9H77ZQp"
      },
      "source": [
        "**Task**: Define `vis_model` to output the feature map specified by `filter_idx` of the layer specified by `layer_idx`.\n",
        "\n",
        "**Hint**: The shape of a convolution layer's output is `(num_samples, width, height, num_filters)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3TyGj0eQByL"
      },
      "outputs": [],
      "source": [
        "def deprocess(img):\n",
        "  return tf.cast( 255*(img[0,:] + 1.) / 2., tf.uint8)\n",
        "\n",
        "def calc_loss(img, model):\n",
        "  '''Compute loss as average activation of the model'''\n",
        "  return K.mean( model(img) )\n",
        "\n",
        "def filter_activation_maximization(model, img, layer_idx, filter_idx, \n",
        "                                   steps=100, \n",
        "                                   step_size=1, \n",
        "                                   vis_steps=100,\n",
        "                                   display=True):\n",
        "\n",
        "    # Define the model\n",
        "    ### START YOUR CODE HERE ### (1 LOC)\n",
        "    vis_model = Model(\n",
        "        model.input,\n",
        "        \n",
        "    )\n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as gtape:\n",
        "            gtape.watch(img)\n",
        "            loss = calc_loss(img, vis_model)\n",
        "\n",
        "            # Compute the gradient of the loss with respect to the input image\n",
        "            grads = gtape.gradient(loss, img)\n",
        "\n",
        "            # Normalize the gradients\n",
        "            grads = K.l2_normalize(grads) + 1e-8\n",
        "\n",
        "            # Perform gradient ascent to make the image increasingly activate the filter\n",
        "            img += grads*step_size\n",
        "            img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "            if ((step+1) % vis_steps == 0) or step+1 == steps:\n",
        "                print('step {} - loss: {:.3g} - avg. gradient: {:.3g}'.format(step+1, loss, np.mean(np.abs(grads))))\n",
        "                if display:\n",
        "                    show( deprocess(img) )\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fglz1X9vEze"
      },
      "source": [
        "Run the cell below to restore a checkpoint where the model was already fine-tuned on our flowers dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e13ZOSHBvEGz"
      },
      "outputs": [],
      "source": [
        "#@title Restore fine-tuned model from checkpoint\n",
        "\n",
        "ckp_path = '/tmp/checkpoints/fi_flowers_vgg16_fine_tuned'\n",
        "os.makedirs(ckp_path, exist_ok=True)\n",
        "DEST_PATH = os.path.join(ckp_path, 'model.zip')\n",
        "\n",
        "# download and unzip\n",
        "!wget -nv -t 0 --show-progress -O $DEST_PATH 'https://cloud.tu-ilmenau.de/s/zW7t9oD9dHJA8jj/download/lab_2.2_ckp.zip'\n",
        "!sleep 1\n",
        "!unzip -uq $DEST_PATH\n",
        "!rm  $DEST_PATH\n",
        "\n",
        "# restore model\n",
        "model.load_weights(os.path.join( os.path.basename(ckp_path), 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_90g_yigwE0X"
      },
      "source": [
        "Evaluation using this checkpoint should return 97% accuracy on the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoJ8HhkVwLjp"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print('\\nloss: {:.4f}\\nacc: {:.4f}'.format(loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgvptTT2CIMr"
      },
      "source": [
        "Now run `filter_activation_maximization` on an image initialized with random noise and visualize the input patterns activating the first twelve filters of the first convolution layer (`layer_idx = 1`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_I_GuDwSoNk"
      },
      "outputs": [],
      "source": [
        "layer_idx = 1\n",
        "num_filters = 12\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(num_filters*2,8)\n",
        "fig.suptitle( model.layers[ layer_idx ].name )\n",
        "\n",
        "for filter_idx in range(num_filters):\n",
        "    filter_idx_offset = 0\n",
        "    img = filter_activation_maximization(\n",
        "        model, \n",
        "        tf.cast( np.random.random((1, 150, 150, 3))*.1 +.3 , tf.float32 ),\n",
        "        layer_idx, \n",
        "        filter_idx + filter_idx_offset,\n",
        "        steps=200,\n",
        "        display=False\n",
        "    )\n",
        "\n",
        "    sp = plt.subplot(num_filters // 6, 6, filter_idx+1)\n",
        "    sp.axis('Off')\n",
        "    sp.title.set_text(str(filter_idx+filter_idx_offset))\n",
        "    plt.imshow(deprocess(img))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRfL1ngcUpp"
      },
      "source": [
        "Filter of later layers activate more abstract high-level concepts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s445M6TaDC4"
      },
      "outputs": [],
      "source": [
        "# Define the layer to be activated\n",
        "layer_idx = 17 # corresponds to layer 'block5_conv3'\n",
        "\n",
        "# Define the layer's filter to be activated\n",
        "# Try out different filters: 3, 9, 17, 136, 157, 160, 168, 179, 185, 199, 202, 222, 224\n",
        "filter_idx = 136 \n",
        "\n",
        "# Init random gray image with some noise\n",
        "img = tf.cast( np.random.random((1, 150, 150, 3))*.1 +.3 , tf.float32 )\n",
        "\n",
        "# Run activation maximization\n",
        "img2 = filter_activation_maximization(model, img, layer_idx, filter_idx, 200, 1, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FNWrd4sUSJz"
      },
      "source": [
        "***\n",
        "\n",
        "# Congratulations!\n",
        "\n",
        "You've learned how to apply **Transfer Learning** and **Fine-tuning** of large models on small datasets. In addition, you learned how to use **learning rate schedules** and **early stopping** as **CallBacks**. At last, you saw how your model can be tweaked to return intermediate results such as feature maps and how to do **activation maximization** for interpreting the filters your model has learned.\n",
        "\n",
        "***"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "__7Oh3RUSUCI",
        "cFyNVcbwj959",
        "-Jbua08yz7Rc",
        "zbzQ41pq16_T",
        "RHiMsmK8RWmU",
        "b4Y7ZcFi1PIV",
        "mmDB2kJXrBLA",
        "nQiBC0svYULU",
        "V9aVjZDju0jk",
        "QPrQCl-d_6Ed",
        "MItS2fVa5r4R",
        "9FNWrd4sUSJz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}